---
title: "TagSeq - from raw reads to gene counts"
author: "Michael Studivan"
date: 
output:
  html_document:
    theme: darkly
    toc: yes
    toc_depth: 3
    toc_float: yes
knit: (function(input, ...) {
    rmarkdown::render(
      input,
      output_file = 'index.html',
      envir = globalenv()
    )
  })
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

<a href="https://github.com/mstudiva/SCTLD-Panama-omics" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#2C3E50; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

```{=html}
<style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
```

#### version: `r library(magrittr)` `r Sys.Date() %>% format(format="%d %B, %Y")`

#### [GitHub repository](https://github.com/mstudiva/SCTLD-Panama-omics){target="_blank"}

## A B O U T &nbsp; T H I S &nbsp; D O C U M E N T

This walkthrough describes the initial processing of TagSeq reads generated from four coral species collected during a stony coral tissue loss disease experiment. It describes (1) setting up the workspace, (2) downloading, unzipping, and concatenating reads, (3) removing Illumina adapters and quality filtering, (4) aligning reads to reference transcriptomes, and (5) generating gene counts matrices. For species-specific, R-based analysis pipelines for differential expression, please see below:

- [*Acropora cervicornis*](acer/)
- [*Orbicella faveolata*](ofav/)
- [*Pavona clavus*](pcla/)
- [*Pocillopora* sp.](poci/)

Library prep, bioinformatics, and analysis protocols are credited to the TagSeq pipeline originally developed by Eli Meyer and Misha Matz: https://doi.org/10.1111/j.1365-294X.2011.05205.x, and further refined by Michael Studivan: https://github.com/mstudiva/tag-based_RNAseq

<br>

***
## S E T U P 
***

Download necessary scripts and load modules for processing/analysis

### Downloading scripts

```{bash, downloading scripts}
cd ~/bin
git clone https://github.com/mstudiva/tag-based_RNAseq
mv tag-based_RNAseq/* .
```

Make all bin scripts executable
```{bash, scripts +x}
chmod +x *.sh *.pl *.py
```

<br>

### Creating conda environments

These packages don't play well with KoKo modules, or they require specific dependencies that conflict with base installations
```{bash, conda}
module load anaconda3-2021.05-gcc-9.4.0-llhdqho
conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge

conda create -n TagSeq bowtie2 samtools

conda create -n cutadapt cutadapt
```

<br>

### Installing Illumina BaseSpace

```{bash, bs dl}
wget "https://launch.basespace.illumina.com/CLI/latest/amd64-linux/bs" -O $HOME/bin/bs

chmod +x ~/bin/bs
```

Go to the website and confirm authorization by logging in to your BaseSpace account
```{bash, bs}
bs auth
```

<br>

***
## D O W N L O A D &nbsp; R E A D S
***

### Downloading and concatenating raw reads
```{bash, wd}
mkdir rawReads
cd rawReads
```

Transfer directly from Illumina BaseSpace
```{bash, download}
echo '#!/bin/bash' > downloadReads.sh
echo 'bs download project --concurrency=high -q -n ####### -o .' >> downloadReads.sh
# -n is the BaseSpace project name and -o is the output directory

echo "find . -name '*.gz' -exec mv {} . \;" >> downloadReads.sh
echo 'rmdir SA*' >>downloadReads.sh
echo 'mkdir ../concatReads' >> downloadReads.sh
echo 'cp *.gz ../concatReads' >> downloadReads.sh
echo 'cd ../concatReads' >> downloadReads.sh

# NOTE: ONLY INCLUDE ABOVE LINE IF YOU HAVE '-2' IN YOUR FILENAMES: this removes the '-2' in filenames from duplicate samples for downstream merging
echo 'for file in *.gz; do mv "${file}" "${file/-2/}"; done' >> downloadReads.sh

# NOTE: This replaces any sample numbers with the three digit version
echo "for file in *.fastq.gz; do echo $file | awk -F_ '{ printf("%03d_%s\n", $1, substr($0, index($0, $2))); }' | xargs -I{} mv $file {}; done" >> downloadReads.sh

echo 'mergeReads.sh -o mergeTemp' >> downloadReads.sh
# -o is the directory to put output files in

echo 'rm *L00*' >> downloadReads.sh
echo "find . -name '*.gz' -exec mv {} . \;" >> downloadReads.sh
echo 'gunzip *.gz' >> downloadReads.sh
echo 'rmdir mergeTemp' >> downloadReads.sh

chmod +x downloadReads.sh

launcher_creator.py -b 'srun downloadReads.sh' -n downloadReads -q shortq7 -t 06:00:00 -e studivanms@gmail.com
sbatch --mem=200GB downloadReads.slurm

# NOTE: This script may stall at the gunzip stage - run this if so:
for F in *.gz; do echo "gunzip $F" >> unzip; done
launcher_creator.py -j unzip -n unzip -q shortq7 -t 6:00:00 -e studivanms@gmail.com
sbatch unzip.slurm
```

Counting raw reads
```{bash, countraw}
# Double check you have the correct number of files as samples
ll *.fastq | wc -l

# Look at the reads
head -50 SampleName.fastq
# Note that every read has four lines, the ID line starts with @HWI

# Shows only nucleotide sequences in file
head -100 SampleName.fastq | grep -E '^[NACGT]+$'

echo '#!/bin/bash' >rawReads.sh
echo readCounts.sh -e gz -o Raw >>rawReads.sh
sbatch -o rawReads.o%j -e rawReads.e%j --mail-type=ALL --mail-user=studivanms@gmail.com rawReads.sh
```
scp RawReadCounts to local machine

<br>

***
## Q U A L I T Y &nbsp; F I L T E R I N G &nbsp; & &nbsp; T R I M M I N G
***

### Removing Illumina headers and quality filtering with cutadapt
Deduplicates row pools into separate 3ill-BC's (1-12), using reverse complement as the ID
```{bash, trim}
conda activate cutadapt

echo '#!/bin/bash' > trim
echo 'conda activate cutadaptenv' >> trim
for F in *.fastq; do
echo "tagseq_clipper.pl $F | cutadapt - -a AAAAAAAA -a AGATCGG -q 15 -m 25 -o ${F/.fastq/}.trim" >>trim;
done

launcher_creator.py -j trim -n trim -q shortq7 -t 6:00:00 -e studivanms@gmail.com
sbatch trim.slurm
```

Do we have the correct number of files?
```{bash, check count}
ll *.trim | wc -l

# did the trimming work?
head -100 SampleName.fastq | grep -E '^[NACGT]+$'
head -100 SampleName.trim | grep -E '^[NACGT]+$'
# the long runs of base A should be gone

# double-check that the rnaseq_clipper did not filter out too many reads by looking at the trim.e####### file
# make sure you're not losing too many reads to duplicates
# rename as a txt file
mv trim.e####### trim.txt

echo '#!/bin/bash' >trimmedReads.sh
echo readCounts.sh -e trim -o Trimmed >>trimmedReads.sh
sbatch -o trimmedReads.o%j -e trimmedReads.e%j --mail-type=ALL --mail-user=studivanms@gmail.com trimmedReads.sh
```
scp TrimmedReadCounts to local machine

Housekeeping to archive raw reads and reduce filesize
```{bash, zip}
mv *.fastq ../rawReads/

# Rezips the raw fastq's for storage
zipper.py -f fastq -a -9 --launcher -e studivanms@gmail.com
sbatch --mem=200GB zip.slurm
```

<br>

***
## T R A N S C R I P T O M E &nbsp; A L I G N M E N T & &nbsp; G E N E &nbsp; C O U N T S
***

### *Acropora cervicornis*
Host transcriptome from Locatelli et al. (2024): https://doi.org/10.1186/s12864-024-11025-3 
Symbiont transcriptome from Shoguchi et al. (2018): https://doi.org/10.1186/s12864-018-4857-9
```{bash, downloading acer}
cd ~/db
mkdir acer_symA
cd acer_symA
# Copy your transcriptome fasta's here

# Creating bowtie2 index for multiple transcriptomes
conda activate TagSeq
echo 'bowtie2-build Acervicornis.fasta, Symbiodinium.fasta Acervicornis_Symbiodinium' > btb
launcher_creator.py -j btb -n btb -q shortq7 -t 6:00:00 -e studivanms@gmail.com
sbatch btb.slurm
```

Now map your sequences against the combined reference transcriptome
```{bash, mapping acer}
mkdir ~/concatReads/acer
cd ~/concatReads/acer
mkdir unaligned

conda activate TagSeq
tagseq_bowtie2_launcher_concat.py -g ~/db/Host_concat/Host_concat -f .trim -n maps --split -u un -a al --undir unaligned --launcher -e studivanms@gmail.com
sbatch maps.slurm
```

What was the mapping efficiency?
```{bash, mapped acer}
ll *.sam | wc -l

echo '#!/bin/bash' >mappedReads.sh
echo readCounts.sh -e al -o Mapped >>mappedReads.sh
sbatch -o mappedReads.o%j -e mappedReads.e%j --mail-type=ALL --mail-user=studivanms@gmail.com mappedReads.sh
```
scp MappedReadCounts to local machine

Housekeeping to delete unnecessary files and reduce filesize
```{bash, zip acer}
zipper.py -f trim -a -9 --launcher -e studivanms@gmail.com
sbatch --mem=200GB zip.slurm

rm *.al
rm unaligned/*.un
```

Generating read counts per gene
```{bash, gene counts acer}
cp ~/annotate/Acervicornis/Acervicornis_seq2iso.tab ~/db/acer_symA
cp ~/annotate/Symbiodinium/Symbiodinium_seq2iso.tab ~/db/acer_symA
cd ~/db/acer_symA
cat Acervicornis_seq2iso.tab Symbiodinium_seq2iso.tab Acervicornis_Symbiodinium_seq2iso.tab

cd ~/concatReads/acer
conda activate TagSeq
samcount_launch_bt2.pl '\.sam$' /home/mstudiva/db/acer_symD/Acervicornis_Symbiodinium_seq2iso.tab > sc
launcher_creator.py -j sc -n sc -q shortq7 -t 6:00:00 -e studivanms@gmail.com
sbatch sc.slurm
```

Assembling gene counts into a matrix
```{bash, gene matrix acer}
ll *.counts | wc -l

zipper.py -f sam -a -9 --launcher -e studivanms@gmail.com
sbatch --mem=200GB zip.slurm

srun expression_compiler.pl *.counts > allc_acer.txt
cat allc.txt | perl -pe 's/\.trim\.sam\.counts//g'>allcounts_acer.txt
head allcounts_acer.txt
```
scp allcounts_acer.txt to local machine

<br>

### *Orbicella faveolata*
Host transcriptome from Young et al. (2024): https://doi.org/10.1186/s12864-024-10092-w 
Symbiont transcriptome from Shoguchi et al. (2021): https://doi.org/10.1093/gbe/evaa235
```{bash, downloading ofav}
cd ~/db
mkdir ofav_symD
cd ofav_symD
# Copy your transcriptome fasta's here

# Creating bowtie2 index for multiple transcriptomes
conda activate TagSeq
echo 'bowtie2-build Ofaveolata.fasta, Durusdinium.fasta Ofaveolata_Durusdinium' > btb
launcher_creator.py -j btb -n btb -q shortq7 -t 6:00:00 -e studivanms@gmail.com
sbatch btb.slurm
```

Now map your sequences against the combined reference transcriptome
```{bash, mapping ofav}
mkdir ~/concatReads/ofav
cd ~/concatReads/ofav
mkdir unaligned

conda activate TagSeq
tagseq_bowtie2_launcher_concat.py -g ~/db/Ofaveolata_Durusdinium/Ofaveolata_Durusdinium -f .trim -n maps --split -u un -a al --undir unaligned --launcher -e studivanms@gmail.com
sbatch maps.slurm
```

What was the mapping efficiency?
```{bash, mapped ofav}
ll *.sam | wc -l

echo '#!/bin/bash' >mappedReads.sh
echo readCounts.sh -e al -o Mapped >>mappedReads.sh
sbatch -o mappedReads.o%j -e mappedReads.e%j --mail-type=ALL --mail-user=studivanms@gmail.com mappedReads.sh
```
scp MappedReadCounts to local machine

Housekeeping to delete unnecessary files and reduce filesize
```{bash, zip ofav}
zipper.py -f trim -a -9 --launcher -e studivanms@gmail.com
sbatch --mem=200GB zip.slurm

rm *.al
rm unaligned/*.un
```

Generating read counts per gene
```{bash, gene counts ofav}
cp ~/annotate/Ofaveolata/Ofaveolata_seq2iso.tab ~/db/ofav_symD
cp ~/annotate/Durusdinium/Durusdinium_seq2iso.tab ~/db/ofav_symD
cd ~/db/ofav_symD
cat Ofaveolata_seq2iso.tab Durusdinium_seq2iso.tab Ofaveolata_SDurusdinium_seq2iso.tab

cd ~/concatReads/ofav
conda activate TagSeq
samcount_launch_bt2.pl '\.sam$' /home/mstudiva/db/ofav_symD/Ofaveolata_Durusdinium_seq2iso.tab > sc
launcher_creator.py -j sc -n sc -q shortq7 -t 6:00:00 -e studivanms@gmail.com
sbatch sc.slurm
```

Assembling gene counts into a matrix
```{bash, gene matrix ofav}
ll *.counts | wc -l

zipper.py -f sam -a -9 --launcher -e studivanms@gmail.com
sbatch --mem=200GB zip.slurm

srun expression_compiler.pl *.counts > allc_ofav.txt
cat allc.txt | perl -pe 's/\.trim\.sam\.counts//g'>allcounts_ofav.txt
head allcounts_ofav.txt
```
scp allcounts_ofav.txt to local machine

<br>

### *Pavona clavus*
Host transcriptome from de-novo assembly (unpublished)
Symbiont transcriptome from Chen et al. (2022): https://doi.org/10.3390/microorganisms10081662
```{bash, downloading pcla}
cd ~/db
mkdir pcla_symC
cd pcla_symC
# Copy your transcriptome fasta's here

# Creating bowtie2 index for multiple transcriptomes
conda activate TagSeq
echo 'bowtie2-build Pclavus.fasta, Cladocopium.fasta Pclavus_Cladocopium' > btb
launcher_creator.py -j btb -n btb -q shortq7 -t 6:00:00 -e studivanms@gmail.com
sbatch btb.slurm
```

Now map your sequences against the combined reference transcriptome
```{bash, mapping pcla}
mkdir ~/concatReads/pcla
cd ~/concatReads/pcla
mkdir unaligned

conda activate TagSeq
tagseq_bowtie2_launcher_concat.py -g ~/db/Pclavus_Cladocopium/Pclavus_Cladocopium -f .trim -n maps --split -u un -a al --undir unaligned --launcher -e studivanms@gmail.com
sbatch maps.slurm
```

What was the mapping efficiency?
```{bash, mapped pcla}
ll *.sam | wc -l

echo '#!/bin/bash' >mappedReads.sh
echo readCounts.sh -e al -o Mapped >>mappedReads.sh
sbatch -o mappedReads.o%j -e mappedReads.e%j --mail-type=ALL --mail-user=studivanms@gmail.com mappedReads.sh
```
scp MappedReadCounts to local machine

Housekeeping to delete unnecessary files and reduce filesize
```{bash, zip pcla}
zipper.py -f trim -a -9 --launcher -e studivanms@gmail.com
sbatch --mem=200GB zip.slurm

rm *.al
rm unaligned/*.un
```

Generating read counts per gene
```{bash, gene counts pcla}
cp ~/annotate/Pclavus/Pclavus_seq2iso.tab ~/db/pcla_symC
cp ~/annotate/Cladocopium/Cladocopium_seq2iso.tab ~/db/pcla_symC
cd ~/db/pcla_symC
cat Pclavus_seq2iso.tab Cladocopium_seq2iso.tab Pclavus_Cladocopium_seq2iso.tab

cd ~/concatReads/pcla
conda activate TagSeq
samcount_launch_bt2.pl '\.sam$' /home/mstudiva/db/pcla_symC/Pclavus_Cladocopium_seq2iso.tab > sc
launcher_creator.py -j sc -n sc -q shortq7 -t 6:00:00 -e studivanms@gmail.com
sbatch sc.slurm
```

Assembling gene counts into a matrix
```{bash, gene matrix pcla}
ll *.counts | wc -l

zipper.py -f sam -a -9 --launcher -e studivanms@gmail.com
sbatch --mem=200GB zip.slurm

srun expression_compiler.pl *.counts > allc_pcla.txt
cat allc.txt | perl -pe 's/\.trim\.sam\.counts//g'>allcounts_pcla.txt
head allcounts_pcla.txt
```
scp allcounts_pcla.txt to local machine

<br>

### *Pocillopora* sp.
Host transcriptome from Buitrago-Lopez et al. (2023): https://doi.org/10.1093/gbe/evaa184
Symbiont transcriptome from Shoguchi et al. (2021): https://doi.org/10.1093/gbe/evaa235
```{bash, downloading poci}
cd ~/db
mkdir poci_symD
cd poci_symD
# Copy your transcriptome fasta's here

# Creating bowtie2 index for multiple transcriptomes
conda activate TagSeq
echo 'bowtie2-build Pocillopora.fasta, Durusdinium.fasta Pocillopora_Durusdinium' > btb
launcher_creator.py -j btb -n btb -q shortq7 -t 6:00:00 -e studivanms@gmail.com
sbatch btb.slurm
```

Now map your sequences against the combined reference transcriptome
```{bash, mapping poci}
mkdir ~/concatReads/poci
cd ~/concatReads/poci
mkdir unaligned

conda activate TagSeq
tagseq_bowtie2_launcher_concat.py -g ~/db/Pocillopora_Durusdinium/Pocillopora_Durusdinium -f .trim -n maps --split -u un -a al --undir unaligned --launcher -e studivanms@gmail.com
sbatch maps.slurm
```

What was the mapping efficiency?
```{bash, mapped poci}
ll *.sam | wc -l

echo '#!/bin/bash' >mappedReads.sh
echo readCounts.sh -e al -o Mapped >>mappedReads.sh
sbatch -o mappedReads.o%j -e mappedReads.e%j --mail-type=ALL --mail-user=studivanms@gmail.com mappedReads.sh
```
scp MappedReadCounts to local machine

Housekeeping to delete unnecessary files and reduce filesize
```{bash, zip poci}
zipper.py -f trim -a -9 --launcher -e studivanms@gmail.com
sbatch --mem=200GB zip.slurm

rm *.al
rm unaligned/*.un
```

Generating read counts per gene
```{bash, gene counts poci}
cp ~/annotate/Pocillopora/Pocillopora_seq2iso.tab ~/db/poci_symD
cp ~/annotate/Durusdinium/Durusdinium_seq2iso.tab ~/db/poci_symD
cd ~/db/poci_symD
cat Pocillopora_seq2iso.tab Durusdinium_seq2iso.tab Pocillopora_Durusdinium_seq2iso.tab

cd ~/concatReads/poci
conda activate TagSeq
samcount_launch_bt2.pl '\.sam$' /home/mstudiva/db/poci_symD/Pocillopora_Durusdinium_seq2iso.tab > sc
launcher_creator.py -j sc -n sc -q shortq7 -t 6:00:00 -e studivanms@gmail.com
sbatch sc.slurm
```

Assembling gene counts into a matrix
```{bash, gene matrix poci}
ll *.counts | wc -l

zipper.py -f sam -a -9 --launcher -e studivanms@gmail.com
sbatch --mem=200GB zip.slurm

srun expression_compiler.pl *.counts > allc_poci.txt
cat allc.txt | perl -pe 's/\.trim\.sam\.counts//g'>allcounts_poci.txt
head allcounts_poci.txt
```
scp allcounts_poci.txt to local machine

<br>

***
## S P E C I E S &nbsp; P I P E L I N E S
***

Now that we have gene count matrices for each species, follow the species-specific differential expression pipelines below:

- [*Acropora cervicornis*](acer/)
- [*Orbicella faveolata*](ofav/)
- [*Pavona clavus*](pcla/)
- [*Pocillopora* sp.](poci/)

<br>